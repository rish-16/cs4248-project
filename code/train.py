# -*- coding: utf-8 -*-
"""[Exp3 FT] Flan-T5-Base + 4 languages

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mA79bEdSFZzBpx1SPwTaXT5U7PHnp8pW
"""

from transformers import DataCollatorForSeq2Seq
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
from huggingface_hub import HfFolder
from nltk.tokenize import sent_tokenize
import numpy as np
import nltk
import evaluate
from transformers import AutoModelForSeq2SeqLM
from datasets import concatenate_datasets
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from datasets import DatasetDict
from datasets import Dataset
from random import randrange
from pprint import pprint
import pandas as pd


DATAPATH = "translated_data/"
MODEL = "flan-t5-large"
FOLDER_NAMES = f"{MODEL}-3lang" # change


def read_translated_data(path, n_samples, lang):
    """
    0: premise
    1: hypothesis
    2: label
    3: explanation
    """
    df = pd.read_csv(path)
    premise = df['premise'].values.tolist()[:n_samples]
    hypo = df['hypothesis'].values.tolist()[:n_samples]
    label = df['label'].values.tolist()[:n_samples]
    expl = df['explanation_1'].values.tolist()[:n_samples]
    langs = [lang for _ in range(n_samples)]
    data = list(zip(premise, hypo, label, expl, langs))
    return data


n_samples_per_lang = 5000  # change accordingly based on performance

languages = {
    "german": read_translated_data(f"{DATAPATH}esnli_train_de.csv", n_samples_per_lang, "german"),
    "english": read_translated_data(f"{DATAPATH}esnli_train_en.csv", n_samples_per_lang, "english"),
    "spanish": read_translated_data(f"{DATAPATH}esnli_train_es.csv", n_samples_per_lang, "spanish"),
    "french": read_translated_data(f"{DATAPATH}esnli_train_fr.csv", n_samples_per_lang, "french"),
    "dutch": read_translated_data(f"{DATAPATH}esnli_train_nl.csv", n_samples_per_lang, "dutch"),
}


idx = randrange(len(languages['english']))
pprint(languages['english'][idx])
print()
pprint(languages['spanish'][idx])

# change
final_train_categories = {
    "english": True,
    "spanish": True,
    "french": True,
    "dutch": False,
    "german": False
}
final_train_dataset = []
for lang, choice in final_train_categories.items():
    if choice:
        final_train_dataset.extend(languages[lang])

print("Number of training samples:", len(final_train_dataset))
assert len(final_train_dataset) % n_samples_per_lang == 0, "Dataset sample mismatch error"

final_test_categories = {
    "english": False,
    "spanish": False,
    "french": False,
    "dutch": False,
    "german": True
}
final_test_dataset = []
for lang, choice in final_test_categories.items():
    if choice:
        final_test_dataset.extend(languages[lang])

print("Number of testing samples:", len(final_test_dataset))
assert len(final_test_dataset) % n_samples_per_lang == 0, "Dataset sample mismatch error"

final_train_dataset_df = pd.DataFrame(data=final_train_dataset, columns=[
                                      "premise", "hypothesis", "label", "explanation_1", "language"])
final_test_dataset_df = pd.DataFrame(data=final_test_dataset, columns=[
                                     "premise", "hypothesis", "label", "explanation_1", "language"])

"""For a given `premise` and `hypothesis`, the input is structured as follows:

```plaintext
Premise: <premise>. Hypothesis: <hypothesis>
```

and the output is structured as follows:

```plaintext
<label>. <explanation>.
```

An example:

**INPUT:**
"Premise: A person on a horse jumps over a broken down airplane. Hypothesis: A person is training his horse for a competition. Please provide a justification for whether the hypothesis is entailed by the premise, contradicted by the premise, or if it cannot be determined."

**OUTPUT:**
"Neutral. The person is not necessarily training his horse."
"""

final_train_dataset_df['combined_premhypo'] = "Premise: " + \
    final_train_dataset_df['premise'] + " Hypothesis: " + final_train_dataset_df['hypothesis']
final_test_dataset_df['combined_premhypo'] = "Premise: " + \
    final_test_dataset_df['premise'] + " Hypothesis: " + final_test_dataset_df['hypothesis']

final_train_dataset_df['combined_target'] = final_train_dataset_df['label'].str.title(
) + ". " + final_train_dataset_df['explanation_1'].str.title()
final_test_dataset_df['combined_target'] = final_test_dataset_df['label'].str.title() + ". " + \
    final_test_dataset_df['explanation_1'].str.title()

# print (final_train_dataset_df.values.tolist()[0][0])
print("input:", final_train_dataset_df.values.tolist()[0][-2])
print("output:", final_train_dataset_df.values.tolist()[0][-1])

print("input:", final_train_dataset_df.values.tolist()[-1][-2])
print("output:", final_train_dataset_df.values.tolist()[-1][-1])


final_train_dataset_hf = Dataset.from_pandas(final_train_dataset_df)
final_test_dataset_hf = Dataset.from_pandas(final_test_dataset_df)


dataset = DatasetDict({"train": final_train_dataset_hf, "test": final_test_dataset_hf})


model_id = f"google/{MODEL}"
tokenizer = AutoTokenizer.from_pretrained(model_id)


tokenised_inputs = concatenate_datasets([
    dataset['train'],
    dataset['test'],
]).map(lambda x: tokenizer(x['combined_premhypo'], truncation=True), batched=True, remove_columns=["combined_premhypo", "premise", "hypothesis", "combined_target"])

max_source_length = max([len(x) for x in tokenised_inputs["input_ids"]])
print(f"Max source length: {max_source_length}")

tokenized_targets = concatenate_datasets([
    dataset['train'],
    dataset['test'],
]).map(lambda x: tokenizer(x["combined_target"], truncation=True), batched=True, remove_columns=["combined_premhypo", "premise", "hypothesis", "combined_target"])

max_target_length = max([len(x) for x in tokenized_targets["input_ids"]])
print(f"Max target length: {max_target_length}")


def preprocess(sample, padding="max_length"):
    def create_prompt(exg):
        item, lang = exg
        # if lang == "english":
        #     prompt = f"""{item} Please provide a justification for whether the hypothesis is entailed by the premise, contradicted by the premise, or if it cannot be determined."""
        # elif lang == "french":
        #     prompt = f"""{item} Veuillez fournir une justification indiquant si l'hypothèse est impliquée par la prémisse, contredite par la prémisse ou si elle ne peut pas être déterminée."""
        # elif lang == "dutch":
        #     prompt = f"""{item} Geef een rechtvaardiging voor de vraag of de hypothese wordt veroorzaakt door de premisse, wordt tegengesproken door de premisse, of dat deze niet kan worden bepaald."""
        # elif lang == "spanish":
        #     prompt = f"""{item} Proporcione una justificación de si la premisa implica la hipótesis, si la premisa la contradice o si no se puede determinar."""
        # elif lang == "german":
        #     prompt = f"""{item} Bitte begründen Sie, ob die Hypothese durch die Prämisse bedingt ist, durch die Prämisse widerlegt wird oder nicht festgestellt werden kann."""
        prompt = f"""{item} Please provide a justification for whether the hypothesis is entailed by the premise, contradicted by the premise, or if it cannot be determined."""
        return prompt

    inputs = [create_prompt(item) for item in zip(sample['combined_premhypo'], sample['language'])]
    print(inputs[:5])

    # tokenize inputs
    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)

    # Tokenize targets with the `text_target` keyword argument
    labels = tokenizer(text_target=sample["combined_target"],
                       max_length=max_target_length, padding=padding, truncation=True)

    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore
    # padding in the loss.
    if padding == "max_length":
        labels["input_ids"] = [
            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
        ]

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=[
                                "premise", "hypothesis", "combined_premhypo", "combined_target", "label"])
print(f"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}")

print(tokenized_dataset['train'].__dict__.keys())


model_id = f"google/{MODEL}"

# load model from the hub
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

nltk.download("punkt")

# Metric
metric = evaluate.load("rouge")
# metric = evaluate.load("accuracy")

# helper function to postprocess text


def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # rougeLSum expects newline after each sentence
    preds = ["\n".join(sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(sent_tokenize(label)) for label in labels]

    return preds, labels


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    result = {k: round(v * 100, 4) for k, v in result.items()}
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result["gen_len"] = np.mean(prediction_lens)
    return result


# we want to ignore tokenizer pad token in the loss
label_pad_token_id = -100
# Data collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=model,
    label_pad_token_id=label_pad_token_id,
    pad_to_multiple_of=8
)


EPOCHS = 3

# Define training args
training_args = Seq2SeqTrainingArguments(
    output_dir=FOLDER_NAMES,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    predict_with_generate=True,
    bf16=True,
    learning_rate=5e-5,
    num_train_epochs=EPOCHS,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    # metric_for_best_model="overall_f1",
)

# Create Trainer instance
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics,
)

print(trainer)

trainer.train()

trainer.save_model(f"saved_models/{FOLDER_NAMES}/")
tokenizer.save_pretrained(f"saved_models/{FOLDER_NAMES}/")

# to ensure there's no loading issues –> doesn't do anything

model.from_pretrained(f"saved_models/{FOLDER_NAMES}/")
tokenizer.from_pretrained(f"saved_models/{FOLDER_NAMES}/")
