# -*- coding: utf-8 -*-
"""[Exp3 Eval] Finetuned  Flan-T5-Base + 4 languages

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v4uAr0LhlTOZiq3oisVhaTz2SCiGmKlh
"""

from pprint import pprint
from sklearn.metrics import classification_report, confusion_matrix
import torch
import pandas as pd
from transformers import AutoModelForSeq2SeqLM
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

DATAPATH = "translated_data"
MODEL = "flan-t5-large"  # change
FOLDER_NAMES = f"{MODEL}-3lang"  # change

EVAL_DE_PATH = f"{DATAPATH}/final_random_2k_de.csv"
MODELPATH = f"saved_models/{FOLDER_NAMES}/"
TOKENIZERPATH = f"saved_models/{FOLDER_NAMES}/"

model_id = f"google/{MODEL}"
tokenizer = AutoTokenizer.from_pretrained(TOKENIZERPATH)

model_id = f"google/{MODEL}"
model = AutoModelForSeq2SeqLM.from_pretrained(MODELPATH).to("cuda")


random_de_df = pd.read_csv(EVAL_DE_PATH).values.tolist()

print(random_de_df[0][0])
print(random_de_df[0][1])
print(random_de_df[0][2])
print(random_de_df[0][3])
print(random_de_df[0][4])
print(random_de_df[0][5])


updated_prompts_de = []
for record in random_de_df:
    prem = record[1]
    hypo = record[2]
    text = f"Premise: {prem} Hypothesis: {hypo} Please provide a justification for whether the hypothesis is entailed by the premise, contradicted by the premise, or if it cannot be determined."
    updated_prompts_de.append(text)


print(updated_prompts_de[0])
print(updated_prompts_de[1])

BS = 50
device = torch.device("cuda")

corrupt = []

mapper = {"neutral": 0, "contradiction": 1, "entailment": 2}
y_pred = []
y_test = [mapper[record[3]] for record in random_de_df]

all_pred = []
count = 0

for i in range(0, len(updated_prompts_de), BS):
    batch = updated_prompts_de[i:i+BS]
    inputs = tokenizer(batch, return_tensors="pt", padding=True)

    outputs = model.generate(
        inputs['input_ids'].to(device),
        attention_mask=inputs['attention_mask'].to(device),
        min_length=1,
        max_new_tokens=40,
        num_beams=4
    )
    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)

    for pred in preds:
        pred_label = pred.split(".")[0].strip().lower()
        pred_expl = pred.split(".")[1].strip().lower()
        if pred_label in ["neutral", "contradiction", "entailment"]:
            y_pred.append(mapper[pred_label])
            if count < 50:
                all_pred.append([
                    pred_label,
                    pred_expl
                ])
                count += 1
        else:
            corrupt.append([pred_label, pred_expl])

print(len(y_pred))
print(len(y_test))

cls_data = list(zip(y_test, y_pred))
cls_data = pd.DataFrame(cls_data, columns=["Actual", "Prediction"])
conf_matrix = confusion_matrix(y_test, y_pred)
cls_report = classification_report(y_test, y_pred)

print(conf_matrix)

print(cls_report)

print(len(all_pred))

to_save = []

for item in zip(random_de_df[:50], all_pred):
    idx, prem, hypo, label, expl, _ = item[0]  # _ is the language [german]
    pred_label, pred_expl = item[1]

    to_save.append([
        prem,
        hypo,
        label,
        pred_label,
        expl,
        pred_expl
    ])

to_save = pd.DataFrame(to_save, columns=["premise", "hypothesis", "label",
                       "pred_label", "explanation", "pred_explanation"])
to_save.to_csv(f"findings/{FOLDER_NAMES}-german-eval-first50.csv")
